<img width="100%" src="https://capsule-render.vercel.app/api?type=waving&color=31688e&height=120&section=header"/>

[![Typing SVG](https://readme-typing-svg.herokuapp.com/?color=31688e&size=35&center=true&vCenter=true&width=1000&lines=BalanÃ§o+de+Dados+e+TÃ©cnicas+Preditivas;A+base+agora+ganha+equilÃ­brio+e+significado;NormalizaÃ§Ã£o,+codificaÃ§Ã£o+e+amostragem+inteligente;Modelagem+de+alto+desempenho!)](https://git.io/typing-svg)

---
![Python](https://img.shields.io/badge/Python-3.11-blue?logo=python)
![Pandas](https://img.shields.io/badge/Pandas-Data%20Analysis-orange?logo=pandas)
![Matplotlib](https://img.shields.io/badge/Matplotlib-Visualization-blue?logo=plotly)
![Seaborn](https://img.shields.io/badge/Seaborn-Statistical%20Graphics-lightblue?logo=seaborn)
![Plotly](https://img.shields.io/badge/Plotly-Interactive%20Graphs-purple?logo=plotly)
![Scikit Learn](https://img.shields.io/badge/Scikit--Learn-ML%20Preprocessing-green?logo=scikitlearn)
![Imbalanced Learn](https://img.shields.io/badge/Imbalanced%20Learn-SMOTE-yellow?logo=python)
![Status](https://img.shields.io/badge/Status-Completed-brightgreen)

<details>

<summary>ğŸ“š SumÃ¡rio</summary>

## SumÃ¡rio do Projeto

- [ğŸ¯ Objetivo](#objetivo)
- [ğŸ”—Jornada do Projeto](#jornada-do-projeto)
- [ğŸ“‚ Estrutura do Projeto](#-estrutura-do-projeto)
- [ğŸ§¾ DicionÃ¡rio de Dados](#-dicionÃ¡rio-de-dados)
- [ğŸ§© Etapa 1 â€” PrÃ© Processamento](#etapa-1---prÃ©-processamento)
- [ğŸ“Š Etapa 2 â€” AnÃ¡lise Univariada e Bivariada](#-etapa-2---anÃ¡lise-univariada-e-bivariada)
- [ğŸ“ˆ Etapa 3 â€” CorrelaÃ§Ã£o Balanceamento e CodificaÃ§Ã£o](#-etapa-3---correlaÃ§Ã£o-balanceamento-e-codificaÃ§Ã£o)
- [ğŸ§­ PrÃ³ximos Passos (Parte 2)](#-prÃ³ximos-passos-parte-2)
- [ğŸ’­ ReflexÃ£o Final](#-reflexÃ£o-final)
- [ğŸ‘¨â€ğŸ’»Autor](#autor)
- [ğŸ“¦ InstalaÃ§Ã£o dos Requisitos](#-instalaÃ§Ã£o-dos-requisitos)

</details>

# ğŸ§® Projeto de Credit Score - Parte 1 

> Projeto Credit Score - Parte 1
Nesta primeira etapa do projeto Credit Score, construÃ­mos uma base sÃ³lida para compreender o perfil dos clientes e preparar os dados para modelos preditivos de crÃ©dito. O foco Ã© criar um pipeline de dados limpo, balanceado e estatisticamente confiÃ¡vel â€” essencial para anÃ¡lises robustas e machine learning.
> O objetivo Ã© preparar e compreender a base de clientes antes da modelagem, aplicando tÃ©cnicas de prÃ©-processamento, anÃ¡lise univariada e bivariada, e balanceamento de classes.

---

### Objetivo

O termo Credit Score refere-se a uma pontuaÃ§Ã£o numÃ©rica que indica a credibilidade de um indivÃ­duo em relaÃ§Ã£o ao cumprimento de suas obrigaÃ§Ãµes financeiras â€” como emprÃ©stimos e cartÃµes de crÃ©dito.  

O objetivo deste projeto Ã© prever o risco de inadimplÃªncia de clientes com base em atributos demogrÃ¡ficos e financeiros, preparando os dados para uma futura modelagem preditiva.

---

### ğŸ“‚ Estrutura do Projeto  
```markdown

ğŸ“ credit_score_part1/
â”‚
â”œâ”€â”€ data/                         # Base de dados original e tratada
â”œâ”€â”€ img/                          # GrÃ¡ficos gerados nas anÃ¡lises
â”œâ”€â”€ notebooks/                    # Notebooks de processamento
â”‚   â””â”€â”€ credit_score_parte1.ipynb
â”œâ”€â”€ README.md                     # Este arquivo
â””â”€â”€ requirements.txt              # DependÃªncias do projeto


````

### ğŸ”—Jornada do Projeto
<details>
<summary><b>ğŸ”— Jornada do Projeto</b></summary>

```markdown
| Etapa                                | DescriÃ§Ã£o                                                                                                                                                                                                                            |
| -------------------------------------| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| PrÃ©-processamento dos Dados          | Incluiu limpeza, normalizaÃ§Ã£o, padronizaÃ§Ã£o e verificaÃ§Ã£o de missing values, assegurando consistÃªncia e qualidade na base final.                                                                                                       |
| AnÃ¡lise Univariada                   | Exploramos individualmente cada variÃ¡vel, identificando distribuiÃ§Ãµes, outliers e possÃ­veis inconsistÃªncias. Essa etapa permitiu entender o comportamento isolado dos atributos e detectar oportunidades de normalizaÃ§Ã£o e limpeza.    |
| AnÃ¡lise Bivariada                    | Investigamos as relaÃ§Ãµes entre variÃ¡veis e o impacto direto sobre o target (bom ou mau pagador), utilizando grÃ¡ficos e correlaÃ§Ãµes estatÃ­sticas. Essa visÃ£o comparativa ajudou a identificar os atributos com maior poder explicativo. |
| CorrelaÃ§Ã£o entre Atributos           | Geramos uma matriz de correlaÃ§Ã£o para avaliar multicolinearidades e redundÃ¢ncias entre variÃ¡veis, otimizando a base para modelagem futura e reduzindo ruÃ­do informacional.                                                               |
| Tratamento de Atributos CategÃ³ricos  | VariÃ¡veis qualitativas foram transformadas por meio de Label Encoding e *One-Hot Encoding, garantindo compatibilidade com algoritmos de machine learning.                                                                           |
| Balanceamento de Classes             | Aplicamos tÃ©cnicas de oversampling e undersampling (via `imbalanced-learn`) para corrigir o desbalanceamento entre bons e maus pagadores â€” passo essencial para evitar viÃ©s nos modelos futuros.                                     |
| DivisÃ£o em Base de Treino e Teste    | Finalizamos a preparaÃ§Ã£o dividindo o dataset em bases de treino (80%) e teste (20%), estruturando o pipeline para as prÃ³ximas fases de modelagem preditiva.                                                                      |
````
</details>

### ğŸ§¾ DicionÃ¡rio de Dados  
```markdown
| VariÃ¡vel              | DescriÃ§Ã£o                                                                 |
|-----------------------|---------------------------------------------------------------------------|
| **Age**               | Idade do cliente                                                         |
| **Income**            | Renda mensal                                                             |
| **Gender**            | GÃªnero do cliente                                                        |
| **Education**         | NÃ­vel de escolaridade                                                    |
| **Marital**           | Estado civil                                                             |
| **Number of Children**| Quantidade de filhos                                                     |
| **Home**              | Tipo de residÃªncia (alugada ou prÃ³pria)                                  |
| **Credit Score**      | Score de crÃ©dito (variÃ¡vel-alvo)                                         |

````
---

## Etapa 1 - PrÃ© Processamento  

### ğŸ”¹ AÃ§Ãµes Realizadas  
- VerificaÃ§Ã£o de **tipos de dados** e conversÃµes necessÃ¡rias.  
- Tratamento de **valores nulos e inconsistentes**, com justificativas documentadas.  
- IdentificaÃ§Ã£o e correÃ§Ã£o de **valores categÃ³ricos incorretos**.  

### ğŸ’¡ ObservaÃ§Ã£o  
Foi aplicada **normalizaÃ§Ã£o** na variÃ¡vel *Income* (Renda), utilizando `MinMaxScaler`, para adequaÃ§Ã£o Ã  modelagem futura.

```python
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df["Income_Scaled"] = scaler.fit_transform(df[["Income"]])

```
## ğŸ“Š Etapa 2 - AnÃ¡lise Univariada e Bivariada

<details>
<summary><b>ğŸ“ˆ AnÃ¡lise Univariada</b></summary>

### ğŸ”¸ Credit Score (Score de CrÃ©dito)

* A maioria dos clientes possui **score "High"**, indicando perfil de baixo risco.
* Scores â€œAverageâ€ e â€œLowâ€ representam menor parcela, exigindo atenÃ§Ã£o especial na modelagem.

### ğŸ”¸ Age (Idade)

* DistribuiÃ§Ã£o simÃ©trica entre **28 e 45 anos**, mediana â‰ˆ 36.
* Sem outliers significativos.

### ğŸ”¸ Home Ownership (Tipo de Moradia)

* PredominÃ¢ncia de **casas prÃ³prias**, reforÃ§ando estabilidade financeira.

### ğŸ”¸ Income (Renda)

* DistribuiÃ§Ã£o **enviesada Ã  direita (skewed right)**.
* Renda concentrada entre **40k e 100k**, com cauda longa de altos rendimentos.
* RecomendaÃ§Ã£o: **normalizaÃ§Ã£o ou transformaÃ§Ã£o logarÃ­tmica** para uso em modelos ML.

</details>

---

<details>
<summary><b>ğŸ”— AnÃ¡lise Bivariada</b></summary>

| Pergunta                                           | Insight                                            |
| -------------------------------------------------- | -------------------------------------------------- |
| **Existe relaÃ§Ã£o entre a idade e o status civil?** | Sim. Clientes casados tendem a ser mais velhos.    |
| **Qual a relaÃ§Ã£o entre score e escolaridade?**     | Maior escolaridade â†’ score mais alto.              |
| **O salÃ¡rio influencia no score de crÃ©dito?**      | Renda maior â†’ tendÃªncia a score â€œHighâ€.            |
| **Clientes com casa prÃ³pria tÃªm score mais alto?** | Sim. 98,2% dos proprietÃ¡rios possuem score â€œHighâ€. |



</details>


### ğŸ§  ConclusÃ£o da Etapa

> A estabilidade financeira e domÃ©stica (renda e moradia prÃ³pria) sÃ£o os **principais preditores de baixo risco**.

---

## ğŸ“ˆ Etapa 3 - CorrelaÃ§Ã£o, Balanceamento e CodificaÃ§Ã£o

### ğŸ”¹ CorrelaÃ§Ã£o NumÃ©rica

A relaÃ§Ã£o entre **Age** e **Income** apresentou correlaÃ§Ã£o mÃ©dia-alta (â‰ˆ 0.69).

> ğŸ’¬ Justificativa: o aumento da idade reflete progressÃ£o profissional e aumento da renda â€” padrÃ£o esperado em bases financeiras.

### ğŸ”¹ CodificaÃ§Ã£o CategÃ³rica

* **One-Hot Encoding:** Gender, Home Ownership, Marital Status
* **Label Encoding:** Education

### ğŸ”¹ Balanceamento das Classes

A variÃ¡vel *Credit Score* estava **desbalanceada**:

* â€œAverageâ€ â†’ ~70%
* â€œLowâ€ â†’ ~20%
* â€œHighâ€ â†’ ~10%

Foi aplicado **SMOTE** apenas na base de treino para equilibrar as classes.

```python
from imblearn.over_sampling import SMOTE

X_res, y_res = SMOTE(random_state=42).fit_resample(X_train, y_train)
print(Counter(y_res))
```

> ğŸ§  Resultado: melhor distribuiÃ§Ã£o entre classes, reduzindo viÃ©s do modelo e garantindo aprendizado equilibrado.

---

## ğŸ§­ PrÃ³ximos Passos (Parte 2)

ğŸ”¹ Construir e treinar modelos de classificaÃ§Ã£o supervisionada:

* Logistic Regression
* Random Forest
* XGBoost

ğŸ”¹ Avaliar mÃ©tricas:

* Accuracy, Precision, Recall e F1-score
* Matriz de confusÃ£o
* AUC-ROC

ğŸ”¹ Interpretar a importÃ¢ncia das variÃ¡veis e gerar **insights preditivos** sobre o comportamento dos clientes.

---

## ğŸ’­ ReflexÃ£o Final

> O projeto demonstrou que **estabilidade financeira e social** (moradia prÃ³pria, renda alta e escolaridade) sÃ£o fatores decisivos na credibilidade de crÃ©dito.
> Essa compreensÃ£o Ã© essencial para bancos e fintechs que desejam otimizar decisÃµes de concessÃ£o de crÃ©dito.

---

## Autor


<p align="center">
  <b>Johnny Sorato Martins Fernandes</b><br>
  <sub>Consultoria de NegÃ³cios | Cientista de Dados| Analista de Dados - AutomaÃ§Ã£o de Processos - SaaS</sub><br><br>
  <sub> JS Fernandes Consultoria Empresarial - Unidade Primavera do Leste</sub><br><br>
  ğŸ“§ fernandesjohnnys@gmail.com &nbsp;&nbsp;ğŸ“ (66) 99232-1719
</p>

---

## ğŸ“¦ InstalaÃ§Ã£o dos Requisitos

```bash
pip install -r requirements.txt
```

---

<p align="center">
  <i>â€œDados bem tratados contam histÃ³rias, revelam padrÃµes e constroem decisÃµes inteligentes.â€</i>
</p>
```

---
